{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0dc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Liangying, 4/24/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc2e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b269517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP: \n",
      "[0.   0.17 0.33 0.5  0.67 0.83 0.  ]\n",
      "MC: \n",
      "[0.   0.08 0.17 0.27 0.34 0.37 0.  ]\n",
      "TD: \n",
      "[0.   0.33 0.45 0.5  0.55 0.65 0.  ]\n"
     ]
    }
   ],
   "source": [
    "class Random_walk():\n",
    "    def __init__(self, n_states, prob):\n",
    "        self.n_states = n_states \n",
    "        self.prob = prob\n",
    "        self.count_one_episode = np.zeros(self.n_states)\n",
    "        self.path = []\n",
    "        #self.values = np.zeros(self.n_states)\n",
    "        self.values = np.full(self.n_states, 0.5)\n",
    "        self.values[0] = self.values[self.n_states - 1] = 0   # 所有终止态的value都为0\n",
    "    \n",
    "    def reset_MC(self):\n",
    "        #self.values = np.zeros(self.n_states)\n",
    "        self.values = np.full(self.n_states, 0.5)\n",
    "        self.values[0] = self.values[self.n_states - 1] = 0\n",
    "        self.count_one_episode = np.zeros(self.n_states)\n",
    "        self.path = []\n",
    "        \n",
    "    def reset_TD(self):   # TD中values是互相依赖的，不能在每一幕中初始化\n",
    "        self.count_one_episode = np.zeros(self.n_states)\n",
    "        self.path = []\n",
    "        \n",
    "    def End_state_check(self, state):\n",
    "        if state == self.n_states - 1 or state == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def Action(self, state):\n",
    "        #if not self.End_state_check(state):   # 类中方法调用类中其他方法的其中一种写法\n",
    "        return state - 1, state + 1\n",
    "        \n",
    "    def Value_update_DP(self, next_states):\n",
    "        v = 0\n",
    "        for next_state in next_states:\n",
    "            if next_state == self.n_states - 1:\n",
    "                r = 1\n",
    "            else:\n",
    "                r = 0\n",
    "            v += self.prob * (r + self.values[next_state])\n",
    "        return v\n",
    "   \n",
    "    def Value_update_TD(self, alpha, discount, state, next_state):\n",
    "        v = 0\n",
    "        if next_state == self.n_states - 1:\n",
    "            r = 1\n",
    "        else:\n",
    "            r = 0\n",
    "        v = r + discount * self.values[next_state]\n",
    "        self.values[state] += alpha * (v - self.values[state])\n",
    "        \n",
    "    \n",
    "    def Return_update_MC(self, discount, state, path):\n",
    "        G = np.zeros(self.n_states)\n",
    "        if state == self.n_states - 1:\n",
    "            path.append(state)    #将终止态加入路径\n",
    "            G[state] = 1\n",
    "            T = len(path)\n",
    "            for i in range(T-2, -1, -1):\n",
    "                s = path[i]\n",
    "                s_next = path[i+1]\n",
    "                G[s] = 0 + discount * G[s_next]    # 注意，不需要做判断，因为从后往前计算时，相当于就地更新，最后的就是我们一开始的第一个态\n",
    "        return G\n",
    "    \n",
    "\n",
    "\n",
    "def DP():   # policy_evaluation_in_place\n",
    "    rw = Random_walk(7, 0.5)\n",
    "    while(True):\n",
    "        values_old = rw.values.copy()\n",
    "        for state in range(rw.n_states):\n",
    "            if not rw.End_state_check(state):\n",
    "                next_states = rw.Action(state)   # 自动成为数组接收来自函数的多个返回值\n",
    "                rw.values[state] = rw.Value_update_DP(next_states)\n",
    "        delta = np.abs(values_old - rw.values)\n",
    "        \n",
    "        if np.max(delta) < 1e-4:\n",
    "            print(\"DP: \")\n",
    "            print(np.around(rw.values,2))\n",
    "            break\n",
    "        \n",
    "def MC(n, discount, alpha):\n",
    "    rw = Random_walk(7, 0.5)\n",
    "    count_across_episodes = np.zeros(7)\n",
    "    value_across_episodes = np.zeros(7)\n",
    "    for i in range(n):\n",
    "        rw.reset_MC()  # MC不像DP或者TD, 其状态不依赖于上一轮迭代得到的状态，所以每一幕都要初始化\n",
    "        state = np.random.randint(1, 5)    # 将初始状态随机化\n",
    "        while(True):\n",
    "            if not rw.End_state_check(state):   # 如果state是终止态，则不需要计数\n",
    "                rw.path.append(state)\n",
    "                rw.count_one_episode[state] += 1    \n",
    "                next_state = np.random.choice(np.array(rw.Action(state)))   \n",
    "                state = next_state\n",
    "            else:\n",
    "                G = rw.Return_update_MC(discount, state, rw.path)\n",
    "                break\n",
    "        count_across_episodes += np.where(rw.count_one_episode > 0, 1, 0)  # 减少for循环的写法\n",
    "        for j in range(7):\n",
    "            if rw.count_one_episode[j] > 0:\n",
    "                #value_across_episodes[j] += (G[j] - value_across_episodes[j]) / count_across_episodes[j]\n",
    "                value_across_episodes[j] += alpha * (G[j] - value_across_episodes[j])\n",
    "    \n",
    "        # value_across_episodes += (G - value_across_episodes) / count_across_episodes    \n",
    "        # 一定要注意, value不能用以数组为单位进行更新。因为如果某一幕中没有出现上一幕的状态，那么这些状态的value应该是不变的。但是以数组为单位就意味着它们也被更新了。\n",
    "    \n",
    "    print(\"MC: \")\n",
    "    print(np.around(value_across_episodes,2))\n",
    "    \n",
    "    \n",
    "def TD(n, discount, alpha):\n",
    "    rw = Random_walk(7, 0.5)\n",
    "    for i in range(n):      # TD具有DP的性质，需要在每一轮迭代的基础上继续更新，所以不能初始化\n",
    "        rw.reset_TD()\n",
    "        state = np.random.randint(1,5)\n",
    "        while(True):\n",
    "            if not rw.End_state_check(state):  \n",
    "                rw.path.append(state)\n",
    "                next_state = np.random.choice(np.array(rw.Action(state)))\n",
    "                rw.Value_update_TD(alpha, discount, state, next_state)\n",
    "                state = next_state\n",
    "            else:\n",
    "                break\n",
    "    print(\"TD: \")\n",
    "    print(np.round(rw.values, 2))    # 这里的2千万不能少\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DP()\n",
    "    MC(100, 1, 0.01)    # 注意discount与alpha的区别\n",
    "    TD(100, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607f881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter_3.9] *",
   "language": "python",
   "name": "conda-env-jupyter_3.9-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
